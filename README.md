# Mini Reasoning LLM

Mini Reasoning LLM is a playful yet production-ready demonstration of how to pair a FastAPI + PyTorch backend with a modern React frontend to build a reasoning-focused web application. The project showcases a tiny character-level neural network that recognises simple reasoning tasksâ€”single digit addition, parity checks, and word reversalsâ€”and then responds with a chain-of-thought style explanation.

## âœ¨ Overview

- **What it is:** A full-stack demo where users submit a prompt and receive a step-by-step reasoning answer generated by a PyTorch model combined with lightweight symbolic logic.
- **Model superpowers:**
  - Recognises prompts asking for **addition**, **even/odd classification**, or **word reversal**.
  - Explains its steps clearly, mimicking chain-of-thought reasoning.
  - Learns on first run from a curated synthetic dataset, then reuses saved weights for instant responses.
- **Model limitations:**
  - Focuses on simple tasks; outside of the three supported skills it will politely refuse.
  - The vocabulary and training data are intentionally tiny, so phrasing far from the supplied examples may be misclassified.
  - Runs entirely on CPU, optimised for clarity over speed at scale.
- **Tech stack:**
  - Backend: **Python**, **FastAPI**, **PyTorch**.
  - Frontend: **React** (Vite).
  - Deployment ready for **Vercel** (frontend) and any container-friendly host for the backend.

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP POST       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ React Frontendâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ FastAPI /api/complete     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚  â€¢ Input validation       â”‚
        â–²                              â”‚  â€¢ ReasoningService       â”‚
        â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â”‚                                           â–¼
        â”‚                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚ PyTorch TinyReasoningModel â”‚
        â”‚                             â”‚  â€¢ Character GRU           â”‚
        â”‚                             â”‚  â€¢ Task classification     â”‚
        â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â”‚                                           â–¼
        â”‚                             Chain-of-thought generator
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

1. The React client collects a prompt and calls `POST /api/complete`.
2. FastAPI validates the payload and forwards it to `ReasoningService`.
3. `ReasoningService` asks the PyTorch model to classify the task, applies symbolic reasoning to compute the answer, and returns an explanatory completion.
4. The frontend displays the formatted reasoning, including model confidence in its task classification.

## ğŸ›  Setup Guide

### Requirements

- **Python** â‰¥ 3.10
- **Node.js** â‰¥ 18
- **npm** â‰¥ 9 (bundled with Node 18+)

### 1. Clone the repository

```bash
git clone https://github.com/your-org/mini-reasoning-llm.git
cd mini-reasoning-llm
```

### 2. Backend setup

```bash
cd backend
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env  # optional â€“ tweak defaults if desired
```

The first time you run the backend the tiny model trains itself on a synthetic dataset and saves weights to `backend/app/models/artifacts/tiny_reasoning_model.pt`. Subsequent runs load the weights instantly.

### 3. Run the backend locally

```bash
uvicorn app.main:app --reload
```

This starts the FastAPI server at `http://localhost:8000`. The API exposes:
- `GET /health` for a lightweight health check.
- `POST /api/complete` for reasoning completions.

### 4. Frontend setup

```bash
cd ../frontend
cp .env.example .env.local  # configure VITE_API_BASE_URL if backend runs elsewhere
npm install
```

### 5. Run the frontend locally

```bash
npm run dev
```

Visit `http://localhost:5173` in your browser. The app proxies requests directly to the backend URL defined in `VITE_API_BASE_URL`.

## ğŸ“¡ API Documentation

### `POST /api/complete`

- **Request body**
  ```json
  {
    "prompt": "Can you explain 2 plus 3 step by step?"
  }
  ```
- **Successful response**
  ```json
  {
    "completion": "Let's solve this addition carefully.\n1. Identify the numbers: 2 and 3.\n2. Add them together: 2 + 3 = 5.\n3. Therefore, the final answer is 5.\n(model confidence in task type: 92.4%)"
  }
  ```
- **Error responses**
  - `422 Unprocessable Entity` â€“ payload fails validation (e.g., empty prompt).
  - `500 Internal Server Error` â€“ unexpected runtime issue, with diagnostic detail in the `detail` field.

### Input validation

`prompt` is required and limited to 512 characters. All validation is performed by Pydantic before touching the model.

## ğŸš€ Deployment Guide

### Frontend on Vercel

1. Push this repository to GitHub.
2. Create a new Vercel project and import the repo.
3. Vercel reads `vercel.json` to run `npm install && npm run build` inside `frontend/` and serve `frontend/dist`.
4. Define the environment variable `api_base_url` in Vercel â†’ Project Settings â†’ Environment Variables. Vercel injects it into `VITE_API_BASE_URL` during build.

### Backend hosting options

- **Render / Railway / Fly.io / Azure App Service / etc.**
  - Provision a Python service using the `backend/` folder.
  - Install dependencies with `pip install -r backend/requirements.txt`.
  - Run with `uvicorn app.main:app --host 0.0.0.0 --port 8000`.
  - Ensure persistent storage (or fallback to ephemeral training each boot) so the model can store `tiny_reasoning_model.pt`.
- **Container deployment**
  - Create a Dockerfile referencing `backend/` (not included to keep the template lightweight).
  - Expose port `8000`.

### Connecting frontend and backend

- Once the backend has a public URL (e.g., `https://mini-reasoning-backend.onrender.com`), set `VITE_API_BASE_URL` to that URL:
  - Locally: create `frontend/.env.local` containing `VITE_API_BASE_URL=https://...`
  - Vercel: set the `api_base_url` secret to the same value so builds use it automatically.

## ğŸ› Customisation

- **Extend training data:** Edit `backend/app/data/training_samples.py` to add more prompt variations or new task labels. Update `TASK_LABELS` in `backend/app/models/reasoning_model.py` and adjust the service to handle new tasks.
- **Swap the model:** Replace `TinyReasoningModel` with a larger Transformer or load a pre-trained network. Keep the `ReasoningService` contract intact to avoid frontend changes.
- **Modify reasoning templates:** Tweak the chain-of-thought messages in `backend/app/services/reasoning_service.py` to alter tone, add citations, or include extra metadata.
- **Frontend enhancements:**
  - Adjust styles in `frontend/src/styles/App.css`.
  - Break out reusable components under `frontend/src/components/`.
  - Add history, prompt presets, or syntax highlighting.
- **Scaling tips:**
  - Mount persistent storage for model weights in production.
  - Enable CORS restrictions via `CORS_ALLOWED_ORIGINS` in `backend/.env`.
  - Deploy the backend behind a load balancer with auto-scaling if demand grows.

## ğŸ§° Troubleshooting & FAQ

| Issue | Cause | Fix |
| --- | --- | --- |
| `POST /api/complete` returns 422 | Prompt missing or too long | Ensure the request body includes a non-empty `prompt` â‰¤ 512 chars |
| Frontend shows CORS error | Browser blocked cross-origin request | Update `CORS_ALLOWED_ORIGINS` in backend `.env` to include the frontend origin |
| Model takes ~1s on first request | Weights not initialised | Wait for the initial training epoch to finish; subsequent runs load cached weights instantly |
| Vercel build cannot reach backend | Backend URL missing | Set `VITE_API_BASE_URL` (locally) or the `api_base_url` environment variable (Vercel) to your backend URL |
| Backend process lacks write permission | Hosting provider restricts file system | Set `MODEL_WEIGHTS_PATH` to a writable directory or pre-package the weights |

## ğŸ“œ License

This project is released under the [MIT License](LICENSE). Enjoy hacking on your own mini reasoning companion!
